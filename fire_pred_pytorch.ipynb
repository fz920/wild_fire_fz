{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing the libraries\n",
    "import numpy as np\n",
    "\n",
    "# for reading and displaying images\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvLSTMCell(nn.Module):\n",
    "\n",
    "    def __init__(self, input_dim, hidden_dim, kernel_size, bias):\n",
    "        \"\"\"\n",
    "        Initialize ConvLSTM cell.\n",
    "        Parameters\n",
    "        ----------\n",
    "        input_dim: int\n",
    "            Number of channels of input tensor.\n",
    "        hidden_dim: int\n",
    "            Number of channels of hidden state.\n",
    "        kernel_size: (int, int)\n",
    "            Size of the convolutional kernel.\n",
    "        bias: bool\n",
    "            Whether or not to add the bias.\n",
    "        \"\"\"\n",
    "\n",
    "        super(ConvLSTMCell, self).__init__()\n",
    "\n",
    "        self.input_dim = input_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "\n",
    "        self.kernel_size = kernel_size\n",
    "        self.padding = kernel_size[0] // 2, kernel_size[1] // 2\n",
    "        self.bias = bias\n",
    "\n",
    "        self.conv = nn.Conv2d(in_channels=self.input_dim + self.hidden_dim,\n",
    "                              out_channels=4 * self.hidden_dim,\n",
    "                              kernel_size=self.kernel_size,\n",
    "                              padding=self.padding,\n",
    "                              bias=self.bias)\n",
    "\n",
    "    def forward(self, input_tensor, cur_state):\n",
    "        h_cur, c_cur = cur_state\n",
    "\n",
    "        combined = torch.cat([input_tensor, h_cur], dim=1)  # concatenate along channel axis\n",
    "\n",
    "        combined_conv = self.conv(combined)\n",
    "        cc_i, cc_f, cc_o, cc_g = torch.split(combined_conv, self.hidden_dim, dim=1)\n",
    "        i = torch.sigmoid(cc_i)\n",
    "        f = torch.sigmoid(cc_f)\n",
    "        o = torch.sigmoid(cc_o)\n",
    "        g = torch.tanh(cc_g)\n",
    "\n",
    "        c_next = f * c_cur + i * g\n",
    "        h_next = o * torch.tanh(c_next)\n",
    "\n",
    "        return h_next, c_next\n",
    "\n",
    "    def init_hidden(self, batch_size, image_size):\n",
    "        height, width = image_size\n",
    "        return (torch.zeros(batch_size, self.hidden_dim, height, width, device=self.conv.weight.device),\n",
    "                torch.zeros(batch_size, self.hidden_dim, height, width, device=self.conv.weight.device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvLSTM(nn.Module):\n",
    "\n",
    "    \"\"\"\n",
    "    Parameters:\n",
    "        input_dim: Number of channels in input\n",
    "        hidden_dim: Number of hidden channels\n",
    "        kernel_size: Size of kernel in convolutions\n",
    "        num_layers: Number of LSTM layers stacked on each other\n",
    "        batch_first: Whether or not dimension 0 is the batch or not\n",
    "        bias: Bias or no bias in Convolution\n",
    "        return_all_layers: Return the list of computations for all layers\n",
    "        Note: Will do same padding.\n",
    "    Input:\n",
    "        A tensor of size B, T, C, H, W or T, B, C, H, W\n",
    "    Output:\n",
    "        A tuple of two lists of length num_layers (or length 1 if return_all_layers is False).\n",
    "            0 - layer_output_list is the list of lists of length T of each output\n",
    "            1 - last_state_list is the list of last states\n",
    "                    each element of the list is a tuple (h, c) for hidden state and memory\n",
    "    Example:\n",
    "        >> x = torch.rand((32, 10, 64, 128, 128))\n",
    "        >> convlstm = ConvLSTM(64, 16, 3, 1, True, True, False)\n",
    "        >> _, last_states = convlstm(x)\n",
    "        >> h = last_states[0][0]  # 0 for layer index, 0 for h index\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, input_dim, hidden_dim, kernel_size, num_layers,\n",
    "                 batch_first=False, bias=True, return_all_layers=False):\n",
    "        super(ConvLSTM, self).__init__()\n",
    "\n",
    "        self._check_kernel_size_consistency(kernel_size)\n",
    "\n",
    "        # Make sure that both `kernel_size` and `hidden_dim` are lists having len == num_layers\n",
    "        kernel_size = self._extend_for_multilayer(kernel_size, num_layers)\n",
    "        hidden_dim = self._extend_for_multilayer(hidden_dim, num_layers)\n",
    "        if not len(kernel_size) == len(hidden_dim) == num_layers:\n",
    "            raise ValueError('Inconsistent list length.')\n",
    "\n",
    "        self.input_dim = input_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.kernel_size = kernel_size\n",
    "        self.num_layers = num_layers\n",
    "        self.batch_first = batch_first\n",
    "        self.bias = bias\n",
    "        self.return_all_layers = return_all_layers\n",
    "\n",
    "        cell_list = []\n",
    "        for i in range(0, self.num_layers):\n",
    "            cur_input_dim = self.input_dim if i == 0 else self.hidden_dim[i - 1]\n",
    "\n",
    "            cell_list.append(ConvLSTMCell(input_dim=cur_input_dim,\n",
    "                                          hidden_dim=self.hidden_dim[i],\n",
    "                                          kernel_size=self.kernel_size[i],\n",
    "                                          bias=self.bias))\n",
    "\n",
    "        self.cell_list = nn.ModuleList(cell_list)\n",
    "\n",
    "    def forward(self, input_tensor, hidden_state=None):\n",
    "        \"\"\"\n",
    "        Parameters\n",
    "        ----------\n",
    "        input_tensor: todo\n",
    "            5-D Tensor either of shape (t, b, c, h, w) or (b, t, c, h, w)\n",
    "        hidden_state: todo\n",
    "            None. todo implement stateful\n",
    "        Returns\n",
    "        -------\n",
    "        last_state_list, layer_output\n",
    "        \"\"\"\n",
    "        if not self.batch_first:\n",
    "            # (t, b, c, h, w) -> (b, t, c, h, w)\n",
    "            input_tensor = input_tensor.permute(1, 0, 2, 3, 4)\n",
    "\n",
    "        b, _, _, h, w = input_tensor.size()\n",
    "\n",
    "        # Implement stateful ConvLSTM\n",
    "        if hidden_state is not None:\n",
    "            raise NotImplementedError()\n",
    "        else:\n",
    "            # Since the init is done in forward. Can send image size here\n",
    "            hidden_state = self._init_hidden(batch_size=b,\n",
    "                                             image_size=(h, w))\n",
    "\n",
    "        layer_output_list = []\n",
    "        last_state_list = []\n",
    "\n",
    "        seq_len = input_tensor.size(1)\n",
    "        cur_layer_input = input_tensor\n",
    "\n",
    "        for layer_idx in range(self.num_layers):\n",
    "\n",
    "            h, c = hidden_state[layer_idx]\n",
    "            output_inner = []\n",
    "            for t in range(seq_len):\n",
    "                h, c = self.cell_list[layer_idx](input_tensor=cur_layer_input[:, t, :, :, :],\n",
    "                                                 cur_state=[h, c])\n",
    "                output_inner.append(h)\n",
    "\n",
    "            layer_output = torch.stack(output_inner, dim=1)\n",
    "            cur_layer_input = layer_output\n",
    "\n",
    "            layer_output_list.append(layer_output)\n",
    "            last_state_list.append([h, c])\n",
    "\n",
    "        if not self.return_all_layers:\n",
    "            layer_output_list = layer_output_list[-1:]\n",
    "            last_state_list = last_state_list[-1:]\n",
    "\n",
    "        return layer_output_list, last_state_list\n",
    "\n",
    "    def _init_hidden(self, batch_size, image_size):\n",
    "        init_states = []\n",
    "        for i in range(self.num_layers):\n",
    "            init_states.append(self.cell_list[i].init_hidden(batch_size, image_size))\n",
    "        return init_states\n",
    "\n",
    "    @staticmethod\n",
    "    def _check_kernel_size_consistency(kernel_size):\n",
    "        if not (isinstance(kernel_size, tuple) or\n",
    "                (isinstance(kernel_size, list) and all([isinstance(elem, tuple) for elem in kernel_size]))):\n",
    "            raise ValueError('`kernel_size` must be tuple or list of tuples')\n",
    "\n",
    "    @staticmethod\n",
    "    def _extend_for_multilayer(param, num_layers):\n",
    "        if not isinstance(param, list):\n",
    "            param = [param] * num_layers\n",
    "        return param"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(304, 512, 512, 15)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = np.load(\"/Users/zhangfengzhe/Desktop/UROP/wild_fire_urop_fz920/fire_data/all_304_fire_combine_all_feature_512_wind_precipitation_new.npy\")\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(304, 512, 512, 14)\n",
      "(304, 512, 512)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "14"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m, n_H, n_W, n_C = data.shape\n",
    "train_x = data[:, :, :, 0:14]\n",
    "train_y = data[:, :, :, 14]\n",
    "print(train_x.shape)\n",
    "print(train_y.shape)\n",
    "len(train_x[1, 1, 1, :])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of training examples = 273\n",
      "number of test examples = 31\n",
      "X_train shape: (273, 512, 512, 13)\n",
      "Y_train shape: (273, 512, 512)\n",
      "X_test shape: (31, 512, 512, 13)\n",
      "Y_test shape: (31, 512, 512)\n"
     ]
    }
   ],
   "source": [
    "train_x, val_x, train_y, val_y = train_test_split(train_x, train_y, test_size = 0.1)\n",
    "\n",
    "print (\"number of training examples = \" + str(train_x.shape[0]))\n",
    "print (\"number of test examples = \" + str(val_x.shape[0]))\n",
    "print (\"X_train shape: \" + str(train_x.shape))\n",
    "print (\"Y_train shape: \" + str(train_y.shape))\n",
    "print (\"X_test shape: \" + str(val_x.shape))\n",
    "print (\"Y_test shape: \" + str(val_y.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "index = 100\n",
    "plt.imshow(data[index, :, :, 14]) #display sample\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "can only concatenate list (not \"int\") to list",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/Users/zhangfengzhe/Desktop/UROP/wild_fire_urop_fz920/fire_predict/fire_pred_pytorch.ipynb Cell 8'\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/zhangfengzhe/Desktop/UROP/wild_fire_urop_fz920/fire_predict/fire_pred_pytorch.ipynb#ch0000007?line=0'>1</a>\u001b[0m model \u001b[39m=\u001b[39m ConvLSTM(input_dim\u001b[39m=\u001b[39;49m[m, \u001b[39m512\u001b[39;49m, \u001b[39m512\u001b[39;49m, \u001b[39m14\u001b[39;49m],\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/zhangfengzhe/Desktop/UROP/wild_fire_urop_fz920/fire_predict/fire_pred_pytorch.ipynb#ch0000007?line=1'>2</a>\u001b[0m                  hidden_dim\u001b[39m=\u001b[39;49m[\u001b[39m64\u001b[39;49m, \u001b[39m64\u001b[39;49m, \u001b[39m128\u001b[39;49m],\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/zhangfengzhe/Desktop/UROP/wild_fire_urop_fz920/fire_predict/fire_pred_pytorch.ipynb#ch0000007?line=2'>3</a>\u001b[0m                  kernel_size\u001b[39m=\u001b[39;49m(\u001b[39m3\u001b[39;49m, \u001b[39m3\u001b[39;49m),\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/zhangfengzhe/Desktop/UROP/wild_fire_urop_fz920/fire_predict/fire_pred_pytorch.ipynb#ch0000007?line=3'>4</a>\u001b[0m                  num_layers\u001b[39m=\u001b[39;49m\u001b[39m3\u001b[39;49m,\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/zhangfengzhe/Desktop/UROP/wild_fire_urop_fz920/fire_predict/fire_pred_pytorch.ipynb#ch0000007?line=4'>5</a>\u001b[0m                  batch_first\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/zhangfengzhe/Desktop/UROP/wild_fire_urop_fz920/fire_predict/fire_pred_pytorch.ipynb#ch0000007?line=5'>6</a>\u001b[0m                  bias\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/zhangfengzhe/Desktop/UROP/wild_fire_urop_fz920/fire_predict/fire_pred_pytorch.ipynb#ch0000007?line=6'>7</a>\u001b[0m                  return_all_layers\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m)\n",
      "\u001b[1;32m/Users/zhangfengzhe/Desktop/UROP/wild_fire_urop_fz920/fire_predict/fire_pred_pytorch.ipynb Cell 3'\u001b[0m in \u001b[0;36mConvLSTM.__init__\u001b[0;34m(self, input_dim, hidden_dim, kernel_size, num_layers, batch_first, bias, return_all_layers)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/zhangfengzhe/Desktop/UROP/wild_fire_urop_fz920/fire_predict/fire_pred_pytorch.ipynb#ch0000001?line=47'>48</a>\u001b[0m \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39m0\u001b[39m, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnum_layers):\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/zhangfengzhe/Desktop/UROP/wild_fire_urop_fz920/fire_predict/fire_pred_pytorch.ipynb#ch0000001?line=48'>49</a>\u001b[0m     cur_input_dim \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39minput_dim \u001b[39mif\u001b[39;00m i \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m \u001b[39melse\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhidden_dim[i \u001b[39m-\u001b[39m \u001b[39m1\u001b[39m]\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/zhangfengzhe/Desktop/UROP/wild_fire_urop_fz920/fire_predict/fire_pred_pytorch.ipynb#ch0000001?line=50'>51</a>\u001b[0m     cell_list\u001b[39m.\u001b[39mappend(ConvLSTMCell(input_dim\u001b[39m=\u001b[39;49mcur_input_dim,\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/zhangfengzhe/Desktop/UROP/wild_fire_urop_fz920/fire_predict/fire_pred_pytorch.ipynb#ch0000001?line=51'>52</a>\u001b[0m                                   hidden_dim\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mhidden_dim[i],\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/zhangfengzhe/Desktop/UROP/wild_fire_urop_fz920/fire_predict/fire_pred_pytorch.ipynb#ch0000001?line=52'>53</a>\u001b[0m                                   kernel_size\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mkernel_size[i],\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/zhangfengzhe/Desktop/UROP/wild_fire_urop_fz920/fire_predict/fire_pred_pytorch.ipynb#ch0000001?line=53'>54</a>\u001b[0m                                   bias\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbias))\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/zhangfengzhe/Desktop/UROP/wild_fire_urop_fz920/fire_predict/fire_pred_pytorch.ipynb#ch0000001?line=55'>56</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcell_list \u001b[39m=\u001b[39m nn\u001b[39m.\u001b[39mModuleList(cell_list)\n",
      "\u001b[1;32m/Users/zhangfengzhe/Desktop/UROP/wild_fire_urop_fz920/fire_predict/fire_pred_pytorch.ipynb Cell 1'\u001b[0m in \u001b[0;36mConvLSTMCell.__init__\u001b[0;34m(self, input_dim, hidden_dim, kernel_size, bias)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/zhangfengzhe/Desktop/UROP/wild_fire_urop_fz920/fire_predict/fire_pred_pytorch.ipynb#ch0000000?line=28'>29</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpadding \u001b[39m=\u001b[39m kernel_size[\u001b[39m0\u001b[39m] \u001b[39m/\u001b[39m\u001b[39m/\u001b[39m \u001b[39m2\u001b[39m, kernel_size[\u001b[39m1\u001b[39m] \u001b[39m/\u001b[39m\u001b[39m/\u001b[39m \u001b[39m2\u001b[39m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/zhangfengzhe/Desktop/UROP/wild_fire_urop_fz920/fire_predict/fire_pred_pytorch.ipynb#ch0000000?line=29'>30</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbias \u001b[39m=\u001b[39m bias\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/zhangfengzhe/Desktop/UROP/wild_fire_urop_fz920/fire_predict/fire_pred_pytorch.ipynb#ch0000000?line=31'>32</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconv \u001b[39m=\u001b[39m nn\u001b[39m.\u001b[39mConv2d(in_channels\u001b[39m=\u001b[39m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49minput_dim \u001b[39m+\u001b[39;49m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mhidden_dim,\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/zhangfengzhe/Desktop/UROP/wild_fire_urop_fz920/fire_predict/fire_pred_pytorch.ipynb#ch0000000?line=32'>33</a>\u001b[0m                       out_channels\u001b[39m=\u001b[39m\u001b[39m4\u001b[39m \u001b[39m*\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhidden_dim,\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/zhangfengzhe/Desktop/UROP/wild_fire_urop_fz920/fire_predict/fire_pred_pytorch.ipynb#ch0000000?line=33'>34</a>\u001b[0m                       kernel_size\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mkernel_size,\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/zhangfengzhe/Desktop/UROP/wild_fire_urop_fz920/fire_predict/fire_pred_pytorch.ipynb#ch0000000?line=34'>35</a>\u001b[0m                       padding\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpadding,\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/zhangfengzhe/Desktop/UROP/wild_fire_urop_fz920/fire_predict/fire_pred_pytorch.ipynb#ch0000000?line=35'>36</a>\u001b[0m                       bias\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbias)\n",
      "\u001b[0;31mTypeError\u001b[0m: can only concatenate list (not \"int\") to list"
     ]
    }
   ],
   "source": [
    "model = ConvLSTM(input_dim=[m, 512, 512, 14],\n",
    "                 hidden_dim=[64, 64, 128],\n",
    "                 kernel_size=(3, 3),\n",
    "                 num_layers=3,\n",
    "                 batch_first=True,\n",
    "                 bias=True,\n",
    "                 return_all_layers=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.01, momentum=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = [train_x, train_y]\n",
    "test_loader = [val_x, val_y]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(512, 512, 13)\n",
      "(512, 512, 13)\n",
      "(512, 512)\n",
      "(512, 512)\n"
     ]
    }
   ],
   "source": [
    "for data in test_loader:\n",
    "    image, label = data[0], data[1]\n",
    "    print(image.shape)\n",
    "    print(label.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "def testAccuracy():\n",
    "    \n",
    "    model.eval()\n",
    "    accuracy = 0.0\n",
    "    total = 0.0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for i in range(m):\n",
    "            images, labels = val_x[i, :, :, :], val_y[i, :, :]\n",
    "            # run the model on the test set to predict labels\n",
    "            outputs = model(images)\n",
    "            # the label with the highest energy will be our prediction\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            accuracy += (predicted == labels).sum().item()\n",
    "    \n",
    "    # compute the accuracy over all test images\n",
    "    accuracy = (100 * accuracy / total)\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.autograd import Variable\n",
    "\n",
    "\n",
    "# Training function. We simply have to loop over our data iterator and feed the inputs to the network and optimize.\n",
    "def train(num_epochs):\n",
    "\n",
    "    # Define your execution device\n",
    "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(\"The model will be running on\", device, \"device\")\n",
    "    # Convert model parameters and buffers to CPU or Cuda\n",
    "    model.to(device)\n",
    "\n",
    "    for epoch in range(num_epochs):  # loop over the dataset multiple times\n",
    "        running_loss = 0.0\n",
    "\n",
    "        for i in range(m):\n",
    "            images = torch.tensor(train_x[i, :, :, :])\n",
    "            labels = torch.tensor(train_y[i, :, :])\n",
    "            \n",
    "            # get the inputs\n",
    "            images = Variable(images.to(device))\n",
    "            labels = Variable(labels.to(device))\n",
    "\n",
    "            # zero the parameter gradients\n",
    "            optimizer.zero_grad()\n",
    "            # predict classes using images from the training set\n",
    "            outputs = model(images)\n",
    "            # compute the loss based on model output and real labels\n",
    "            loss = loss_fn(outputs, labels)\n",
    "            # backpropagate the loss\n",
    "            loss.backward()\n",
    "            # adjust parameters based on the calculated gradients\n",
    "            optimizer.step()\n",
    "\n",
    "            # Let's print statistics for every 1,000 images\n",
    "            running_loss += loss.item()     # extract the loss value\n",
    "            if i % 1000 == 999:    \n",
    "                # print every 1000 (twice per epoch) \n",
    "                print('[%d, %5d] loss: %.3f' %\n",
    "                      (epoch + 1, i + 1, running_loss / 1000))\n",
    "                # zero the loss\n",
    "                running_loss = 0.0\n",
    "\n",
    "        # Compute and print the average accuracy fo this epoch when tested over all 10000 test images\n",
    "        accuracy = testAccuracy()\n",
    "        print('For epoch', epoch+1,'the test accuracy over the whole test set is %d %%' % (accuracy))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The model will be running on cpu device\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "not enough values to unpack (expected 5, got 3)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m/Users/zhangfengzhe/Desktop/UROP/wild_fire_urop_fz920/fire_predict/fire_pred_pytorch.ipynb Cell 14'\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/zhangfengzhe/Desktop/UROP/wild_fire_urop_fz920/fire_predict/fire_pred_pytorch.ipynb#ch0000011?line=0'>1</a>\u001b[0m train(\u001b[39m10\u001b[39;49m)\n",
      "\u001b[1;32m/Users/zhangfengzhe/Desktop/UROP/wild_fire_urop_fz920/fire_predict/fire_pred_pytorch.ipynb Cell 13'\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(num_epochs)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/zhangfengzhe/Desktop/UROP/wild_fire_urop_fz920/fire_predict/fire_pred_pytorch.ipynb#ch0000010?line=24'>25</a>\u001b[0m optimizer\u001b[39m.\u001b[39mzero_grad()\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/zhangfengzhe/Desktop/UROP/wild_fire_urop_fz920/fire_predict/fire_pred_pytorch.ipynb#ch0000010?line=25'>26</a>\u001b[0m \u001b[39m# predict classes using images from the training set\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/zhangfengzhe/Desktop/UROP/wild_fire_urop_fz920/fire_predict/fire_pred_pytorch.ipynb#ch0000010?line=26'>27</a>\u001b[0m outputs \u001b[39m=\u001b[39m model(images)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/zhangfengzhe/Desktop/UROP/wild_fire_urop_fz920/fire_predict/fire_pred_pytorch.ipynb#ch0000010?line=27'>28</a>\u001b[0m \u001b[39m# compute the loss based on model output and real labels\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/zhangfengzhe/Desktop/UROP/wild_fire_urop_fz920/fire_predict/fire_pred_pytorch.ipynb#ch0000010?line=28'>29</a>\u001b[0m loss \u001b[39m=\u001b[39m loss_fn(outputs, labels)\n",
      "File \u001b[0;32m~/Desktop/UROP/PoP_venv/lib/python3.9/site-packages/torch/nn/modules/module.py:1110\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   <a href='file:///Users/zhangfengzhe/Desktop/UROP/PoP_venv/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1105'>1106</a>\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   <a href='file:///Users/zhangfengzhe/Desktop/UROP/PoP_venv/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1106'>1107</a>\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   <a href='file:///Users/zhangfengzhe/Desktop/UROP/PoP_venv/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1107'>1108</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   <a href='file:///Users/zhangfengzhe/Desktop/UROP/PoP_venv/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1108'>1109</a>\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> <a href='file:///Users/zhangfengzhe/Desktop/UROP/PoP_venv/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1109'>1110</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   <a href='file:///Users/zhangfengzhe/Desktop/UROP/PoP_venv/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1110'>1111</a>\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   <a href='file:///Users/zhangfengzhe/Desktop/UROP/PoP_venv/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1111'>1112</a>\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "\u001b[1;32m/Users/zhangfengzhe/Desktop/UROP/wild_fire_urop_fz920/fire_predict/fire_pred_pytorch.ipynb Cell 3'\u001b[0m in \u001b[0;36mConvLSTM.forward\u001b[0;34m(self, input_tensor, hidden_state)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/zhangfengzhe/Desktop/UROP/wild_fire_urop_fz920/fire_predict/fire_pred_pytorch.ipynb#ch0000001?line=69'>70</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbatch_first:\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/zhangfengzhe/Desktop/UROP/wild_fire_urop_fz920/fire_predict/fire_pred_pytorch.ipynb#ch0000001?line=70'>71</a>\u001b[0m     \u001b[39m# (t, b, c, h, w) -> (b, t, c, h, w)\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/zhangfengzhe/Desktop/UROP/wild_fire_urop_fz920/fire_predict/fire_pred_pytorch.ipynb#ch0000001?line=71'>72</a>\u001b[0m     input_tensor \u001b[39m=\u001b[39m input_tensor\u001b[39m.\u001b[39mpermute(\u001b[39m1\u001b[39m, \u001b[39m0\u001b[39m, \u001b[39m2\u001b[39m, \u001b[39m3\u001b[39m, \u001b[39m4\u001b[39m)\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/zhangfengzhe/Desktop/UROP/wild_fire_urop_fz920/fire_predict/fire_pred_pytorch.ipynb#ch0000001?line=73'>74</a>\u001b[0m b, _, _, h, w \u001b[39m=\u001b[39m input_tensor\u001b[39m.\u001b[39msize()\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/zhangfengzhe/Desktop/UROP/wild_fire_urop_fz920/fire_predict/fire_pred_pytorch.ipynb#ch0000001?line=75'>76</a>\u001b[0m \u001b[39m# Implement stateful ConvLSTM\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/zhangfengzhe/Desktop/UROP/wild_fire_urop_fz920/fire_predict/fire_pred_pytorch.ipynb#ch0000001?line=76'>77</a>\u001b[0m \u001b[39mif\u001b[39;00m hidden_state \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "\u001b[0;31mValueError\u001b[0m: not enough values to unpack (expected 5, got 3)"
     ]
    }
   ],
   "source": [
    "train(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "3dc2d4287c3e5df1efb1cfb6ad5e9bbe64ecfebd8ae7d2ff276c7de2f0e96c37"
  },
  "kernelspec": {
   "display_name": "Python 3.9.9 ('PoP_venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
